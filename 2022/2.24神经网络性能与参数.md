>要使神经网络能够更好地记忆数据，它们需要的参数比想象的要多得多。

传统上，只要参数的数量大于要满足的方程数量，我们就可以使用参数化模型来进行数据插值。但在深度学习中，一个令人困惑的现象是，模型训练使用的参数数量比这个经典理论所建议的要多得多。

深度学习中经常会出现各种大型的神经网络，神经网络是执行类人任务的领先 AI 系统。随着它们参数的增多，神经网络已经可以执行各种任务。按照数学的理论，神经网络无需很大就能执行任务，例如在直线 y=2x 中，确定这条直线无需很多参数。但是，**现代神经网络的规模通常远远超出预测的要求，这种情况被称为过度参数化**。

在去年 12 月入选 NeurIPS 的一篇论文《A Universal Law of Robustness via Isoperimetry》中，来自微软研究院的 Sébastien Bubeck 和斯坦福大学的 Mark Sellke 为神经网络扩展成功背后的奥秘提供了新的解释。他们表明，神经网络必须比传统预期规模要大得多，才能避免某些基本问题。这一发现为一个持续了几十年的问题提供了一般性见解。

[论文链接](https://arxiv.org/abs/2105.12806)
