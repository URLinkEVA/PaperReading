>要使神经网络能够更好地记忆数据，它们需要的参数比想象的要多得多。

传统上，只要参数的数量大于要满足的方程数量，我们就可以使用参数化模型来进行数据插值。但在深度学习中，一个令人困惑的现象是，模型训练使用的参数数量比这个经典理论所建议的要多得多。

深度学习中经常会出现各种大型的神经网络，神经网络是执行类人任务的领先 AI 系统。随着它们参数的增多，神经网络已经可以执行各种任务。按照数学的理论，神经网络无需很大就能执行任务，例如在直线 y=2x 中，确定这条直线无需很多参数。但是，**现代神经网络的规模通常远远超出预测的要求，这种情况被称为过度参数化**。

在去年 12 月入选 NeurIPS 的一篇论文《A Universal Law of Robustness via Isoperimetry》中，来自微软研究院的 Sébastien Bubeck 和斯坦福大学的 Mark Sellke 为神经网络扩展成功背后的奥秘提供了新的解释。他们表明，神经网络必须比传统预期规模要大得多，才能避免某些基本问题。这一发现为一个持续了几十年的问题提供了一般性见解。

[论文链接](https://arxiv.org/abs/2105.12806)

瑞士洛桑联邦理工学院的 Lenka Zdeborová 表示：**他们的研究触及了计算机科学的核心**。神经网络规模的标准来自对其如何记忆数据的分析。但要了解数据记忆，我们必须首先了解网络的作用。

神经网络中一项常见的任务是识别图像中的目标。为了创建这种网络，研究人员首先需要提供数据图像及其标注，然后对其进行训练以学习相关参数，之后模型能正确识别图像中的目标。换句话说，训练使网络记住数据。更值得注意的是，**一旦网络记住了足够多的训练数据，它还能够以不同程度的准确率预测它从未见过的目标，这一过程称为泛化**。

网络的大小决定了它可以记住多少。这可以通过图形来理解，想象一下，将两个数据点放在 xy 平面上。你可以将这两个点与由两个参数描述的线连接起来。如果知道了这条线的一个点坐标以及一个原始数据点的 x 坐标，我们只需查看这条线（或使用参数）就可以计算出相应的 y 坐标。因为这条线已经记住了这两个数据点。

神经网络的原理与此类似。例如，图像由成百上千个值描述 —— 每个像素是一个值。**这些值在数学上等价于高维空间中一个点的坐标，坐标的数量称为维度**。

一个古老的数学结果表明，要将 n 个数据点与曲线拟合，则需要一个具有 n 个参数的函数。当神经网络在 1980 年代首次作为一种有影响的力量出现时，研究者的想法与此相同，他们认为**对于神经网络来说，应该只需要 n 个参数来拟合 n 个数据点 —— 无论数据的维度如何**。

德克萨斯大学奥斯汀分校的 Alex Dimakis 表示：「现在的研究改变了，我们经常创建参数数量超过训练样本数量的神经网络。这意味着相关研究文献书籍必须重写。」

Bubeck 和 Sellke 并没有打算重写任何东西。**他们正在研究神经网络缺乏的另一种特性，称为稳健性（robustness），即网络处理微小变化的能力**。例如，一个稳健性较差的网络可能已经学会了识别长颈鹿，但它会将一个几乎没有修改过的图片错误地标记为沙鼠，这就是网络稳健性带来的影响。 

2019 年，当 Bubeck 及其同事意识到该问题与网络规模有关时，他们正在寻求证明有关该问题的定理。新研究表明过度参数化对于网络的稳健性是必要的。他们通过将数据点与曲线拟合所需的参数来做到这一点，**该曲线具有与稳健性等效的数学属性：平滑度。**

为了看到这一点，再次想象平面中的一条曲线，其中 x 坐标代表单个像素的颜色，y 坐标代表图像标签。由于曲线是平滑的，如果你稍微修改像素的颜色，沿着曲线移动一小段距离，相应的预测只会发生很小的变化。另一方面，对于极度锯齿状的曲线，x 坐标（颜色）的微小变化会导致 y 坐标（图像标签）发生剧烈变化，原本识别为长颈鹿的图像可以变成沙鼠。

Bubeck 和 Sellke 的研究表明，平滑拟合高维数据点不仅需要 n 个参数，还需要 n × d 个参数，其中 d 是输入的维度（例如，784 表示 784 像素的图像）。**换句话说，如果你想让神经网络稳健地记住它的训练数据，过度参数化不仅有帮助 —— 而是绝对有效。** 证明依赖于一个关于高维几何的奇特事实 —— 即放置在球体表面上的随机分布的点，几乎都彼此相距一个完整的直径。点与点之间的大间隔意味着用一条平滑曲线拟合它们需要许多额外的参数。

耶鲁大学的 Amin Karbasi 表示，「这个证明非常基础 —— 不需要繁重的数学运算，它说明了一些非常普遍的东西。」

该结果提供了一种新方法来理解为什么扩大神经网络规模的简单策略如此有效。与此同时，其他研究揭示了过度参数化带来帮助的更多原因，例如它可以提高训练过程的效率，以及网络的泛化能力。

虽然我们现在知道过度参数化对于稳健性是必要的，但尚不清楚稳健性对其他事物的必要性。通过将其与过度参数化联系起来，新的证据似乎表明稳健性可能比想象的更加重要，是一个可以带来许多好处的关键因素。

「稳健性似乎是泛化的先决条件，」Bubeck 说到。「如果你有一个系统，你只是稍微扰乱它，然后它就失控了，这会是怎样的系统？这是不可接受的，这是一个非常基础和基本的要求。」

reference：https://news.ycombinator.com/item?id=30288092

https://www.quantamagazine.org/computer-scientists-prove-why-bigger-neural-networks-do-better-20220210/
